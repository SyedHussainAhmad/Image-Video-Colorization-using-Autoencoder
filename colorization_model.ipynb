{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "import numpy as np\n",
        "import cv2\n",
        "from tqdm import tqdm\n",
        "import matplotlib.pyplot as plt\n",
        "from PIL import Image\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "from torch.utils.data import Dataset, DataLoader, random_split\n",
        "import torchvision.transforms as T\n",
        "import torchvision.models as models\n",
        "from torchvision.models import VGG19_Weights\n",
        "import torch.nn.functional as F\n",
        "from sklearn.model_selection import train_test_split\n",
        "from skimage import color\n",
        "from skimage.metrics import peak_signal_noise_ratio as psnr\n",
        "from skimage.metrics import structural_similarity as ssim"
      ],
      "metadata": {
        "id": "Aa4AFxPV9CFz"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Device setup\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "print(\"Using device:\", device)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "olR0moqB9E5i",
        "outputId": "beaeb7fb-2eea-4a5d-c8cc-64da28b529eb"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Using device: cpu\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# DATASET CLASS\n",
        "class ColorizationDataset(Dataset):\n",
        "    def __init__(self, img_dir, size=256, augment=True):\n",
        "        self.img_paths = [os.path.join(img_dir, f) for f in os.listdir(img_dir)\n",
        "                         if f.lower().endswith(('.jpg','.png','.jpeg'))]\n",
        "        self.size = size\n",
        "        self.augment = augment\n",
        "\n",
        "        # Transforms with augmentation\n",
        "        if augment:\n",
        "            self.transform = T.Compose([\n",
        "                T.Resize((size + 32, size + 32)),\n",
        "                T.RandomCrop((size, size)),\n",
        "                T.RandomHorizontalFlip(p=0.5),\n",
        "                T.ColorJitter(brightness=0.1, contrast=0.1, saturation=0.1),\n",
        "                T.ToTensor()\n",
        "            ])\n",
        "        else:\n",
        "            self.transform = T.Compose([\n",
        "                T.Resize((size, size)),\n",
        "                T.ToTensor()\n",
        "            ])\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.img_paths)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        try:\n",
        "            # Load and transform image\n",
        "            img = Image.open(self.img_paths[idx]).convert('RGB')\n",
        "            img = self.transform(img)\n",
        "            img = img.numpy().transpose((1,2,0))\n",
        "\n",
        "            # Convert to LAB with improved normalization\n",
        "            img_lab = color.rgb2lab(img).astype(np.float32)\n",
        "            L = img_lab[:,:,0]  # [0,100]\n",
        "            AB = img_lab[:,:,1:]  # [-128,127]\n",
        "\n",
        "            # Better normalization\n",
        "            L = L / 100.0 * 2.0 - 1.0  # [-1, 1]\n",
        "            AB = AB / 128.0  # [-1, 1]\n",
        "\n",
        "            # Convert to tensors\n",
        "            L_tensor = torch.from_numpy(L).unsqueeze(0)\n",
        "            AB_tensor = torch.from_numpy(AB.transpose((2,0,1)))\n",
        "\n",
        "            return L_tensor.float(), AB_tensor.float()\n",
        "        except Exception as e:\n",
        "            print(f\"Error loading image {self.img_paths[idx]}: {e}\")\n",
        "            # Return a dummy tensor if image fails to load\n",
        "            L_dummy = torch.zeros(1, self.size, self.size)\n",
        "            AB_dummy = torch.zeros(2, self.size, self.size)\n",
        "            return L_dummy, AB_dummy"
      ],
      "metadata": {
        "id": "8wr08NZY9G-8"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# U-NET MODEL\n",
        "class ColorizationModel(nn.Module):\n",
        "    def __init__(self):\n",
        "        super().__init__()\n",
        "\n",
        "        # Encoder: ResNet-50 for better feature extraction\n",
        "        resnet = models.resnet50(weights=models.ResNet50_Weights.IMAGENET1K_V2)\n",
        "\n",
        "        # Extract features at different scales for skip connections\n",
        "        self.encoder1 = nn.Sequential(*list(resnet.children())[:3])   # 64 channels\n",
        "        self.encoder2 = nn.Sequential(*list(resnet.children())[3:5])  # 256 channels\n",
        "        self.encoder3 = nn.Sequential(*list(resnet.children())[5:6])  # 512 channels\n",
        "        self.encoder4 = nn.Sequential(*list(resnet.children())[6:7])  # 1024 channels\n",
        "        self.encoder5 = nn.Sequential(*list(resnet.children())[7:8])  # 2048 channels\n",
        "\n",
        "        # Decoder with skip connections (U-Net style)\n",
        "        self.decoder5 = self._make_decoder_block(2048, 1024)\n",
        "        self.decoder4 = self._make_decoder_block(1024 + 1024, 512)  # +1024 from skip\n",
        "        self.decoder3 = self._make_decoder_block(512 + 512, 256)    # +512 from skip\n",
        "        self.decoder2 = self._make_decoder_block(256 + 256, 128)    # +256 from skip\n",
        "        self.decoder1 = self._make_decoder_block(128 + 64, 64)      # +64 from skip\n",
        "\n",
        "        # Final layer\n",
        "        self.final_conv = nn.Sequential(\n",
        "            nn.Conv2d(64, 32, 3, padding=1),\n",
        "            nn.ReLU(True),\n",
        "            nn.Conv2d(32, 2, 3, padding=1),\n",
        "            nn.Tanh()\n",
        "        )\n",
        "\n",
        "    def _make_decoder_block(self, in_channels, out_channels):\n",
        "        return nn.Sequential(\n",
        "            nn.ConvTranspose2d(in_channels, out_channels, 4, stride=2, padding=1),\n",
        "            nn.BatchNorm2d(out_channels),\n",
        "            nn.ReLU(True),\n",
        "            nn.Conv2d(out_channels, out_channels, 3, padding=1),\n",
        "            nn.BatchNorm2d(out_channels),\n",
        "            nn.ReLU(True)\n",
        "        )\n",
        "\n",
        "    def forward(self, x):\n",
        "        # Convert grayscale to 3-channel\n",
        "        x = x.repeat(1, 3, 1, 1)\n",
        "\n",
        "        # Encoder with skip connections\n",
        "        e1 = self.encoder1(x)     # 64x64x64\n",
        "        e2 = self.encoder2(e1)    # 256x32x32\n",
        "        e3 = self.encoder3(e2)    # 512x16x16\n",
        "        e4 = self.encoder4(e3)    # 1024x8x8\n",
        "        e5 = self.encoder5(e4)    # 2048x4x4\n",
        "\n",
        "        # Decoder with skip connections\n",
        "        d5 = self.decoder5(e5)    # 1024x8x8\n",
        "        d4 = self.decoder4(torch.cat([d5, e4], dim=1))  # 512x16x16\n",
        "        d3 = self.decoder3(torch.cat([d4, e3], dim=1))  # 256x32x32\n",
        "        d2 = self.decoder2(torch.cat([d3, e2], dim=1))  # 128x64x64\n",
        "        d1 = self.decoder1(torch.cat([d2, e1], dim=1))  # 64x128x128\n",
        "\n",
        "        # Final upsampling to 256x256\n",
        "        d1 = F.interpolate(d1, size=(256, 256), mode='bilinear', align_corners=False)\n",
        "\n",
        "        # Final convolution\n",
        "        out = self.final_conv(d1)\n",
        "        return out"
      ],
      "metadata": {
        "id": "Go_0bs3h9Ptz"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# PERCEPTUAL LOSS\n",
        "class PerceptualLoss(nn.Module):\n",
        "    def __init__(self):\n",
        "        super().__init__()\n",
        "        # Use VGG19 for perceptual loss\n",
        "        vgg = models.vgg19(weights=VGG19_Weights.IMAGENET1K_V1).features[:16]\n",
        "        self.vgg = vgg.eval()\n",
        "        for param in self.vgg.parameters():\n",
        "            param.requires_grad = False\n",
        "\n",
        "        self.mse = nn.MSELoss()\n",
        "\n",
        "    def forward(self, pred_lab, target_lab):\n",
        "        try:\n",
        "            # Convert LAB to RGB for VGG\n",
        "            pred_rgb = self.lab_to_rgb_tensor(pred_lab)\n",
        "            target_rgb = self.lab_to_rgb_tensor(target_lab)\n",
        "\n",
        "            # VGG features\n",
        "            pred_features = self.vgg(pred_rgb)\n",
        "            target_features = self.vgg(target_rgb)\n",
        "\n",
        "            # Perceptual loss\n",
        "            perceptual_loss = self.mse(pred_features, target_features)\n",
        "\n",
        "            # Pixel loss\n",
        "            pixel_loss = self.mse(pred_lab, target_lab)\n",
        "\n",
        "            return pixel_loss + 0.1 * perceptual_loss\n",
        "        except Exception as e:\n",
        "            print(f\"Perceptual loss error: {e}\")\n",
        "            # Fallback to pixel loss only\n",
        "            return self.mse(pred_lab, target_lab)\n",
        "\n",
        "    def lab_to_rgb_tensor(self, lab_tensor):\n",
        "        \"\"\"Convert LAB tensor to RGB tensor for VGG\"\"\"\n",
        "        batch_size = lab_tensor.shape[0]\n",
        "        rgb_imgs = []\n",
        "\n",
        "        for i in range(batch_size):\n",
        "            lab = lab_tensor[i].detach().cpu().numpy()\n",
        "\n",
        "            # Denormalize\n",
        "            L = (lab[0] + 1.0) / 2.0 * 100.0\n",
        "            AB = lab[1:] * 128.0\n",
        "\n",
        "            # Combine and convert\n",
        "            lab_img = np.concatenate([L[None], AB], axis=0).transpose(1,2,0)\n",
        "            # Clamp LAB values to valid ranges\n",
        "            lab_img[:,:,0] = np.clip(lab_img[:,:,0], 0, 100)\n",
        "            lab_img[:,:,1:] = np.clip(lab_img[:,:,1:], -128, 127)\n",
        "\n",
        "            rgb_img = color.lab2rgb(lab_img)\n",
        "            rgb_imgs.append(rgb_img)\n",
        "\n",
        "        rgb_tensor = torch.FloatTensor(np.array(rgb_imgs)).permute(0,3,1,2)\n",
        "        return rgb_tensor.to(lab_tensor.device)"
      ],
      "metadata": {
        "id": "IM1NRgCp9Z3q"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# EVALUATION FUNCTION\n",
        "def evaluate_model(model, val_loader, num_samples=20):\n",
        "    \"\"\"Evaluate model with PSNR and SSIM metrics - FIXED VERSION\"\"\"\n",
        "    model.eval()\n",
        "    psnr_scores = []\n",
        "    ssim_scores = []\n",
        "\n",
        "    with torch.no_grad():\n",
        "        sample_count = 0\n",
        "        for L, AB in val_loader:\n",
        "            if sample_count >= num_samples:\n",
        "                break\n",
        "\n",
        "            L, AB = L.to(device), AB.to(device)\n",
        "            pred_AB = model(L)\n",
        "\n",
        "            # Convert to numpy for metrics\n",
        "            for j in range(L.shape[0]):\n",
        "                if sample_count >= num_samples:\n",
        "                    break\n",
        "\n",
        "                try:\n",
        "                    # Reconstruct images\n",
        "                    l_np = L[j,0].cpu().numpy()\n",
        "                    ab_true = AB[j].cpu().numpy()\n",
        "                    ab_pred = pred_AB[j].cpu().numpy()\n",
        "\n",
        "                    # Denormalize\n",
        "                    l_denorm = (l_np + 1.0) / 2.0 * 100.0\n",
        "                    ab_true_denorm = ab_true * 128.0\n",
        "                    ab_pred_denorm = ab_pred * 128.0\n",
        "\n",
        "                    # Clamp values to valid LAB ranges\n",
        "                    l_denorm = np.clip(l_denorm, 0, 100)\n",
        "                    ab_true_denorm = np.clip(ab_true_denorm, -128, 127)\n",
        "                    ab_pred_denorm = np.clip(ab_pred_denorm, -128, 127)\n",
        "\n",
        "                    # Combine LAB\n",
        "                    lab_true = np.stack([l_denorm, ab_true_denorm[0], ab_true_denorm[1]], axis=2)\n",
        "                    lab_pred = np.stack([l_denorm, ab_pred_denorm[0], ab_pred_denorm[1]], axis=2)\n",
        "\n",
        "                    # Convert to RGB\n",
        "                    rgb_true = color.lab2rgb(lab_true)\n",
        "                    rgb_pred = color.lab2rgb(lab_pred)\n",
        "\n",
        "                    # Ensure valid range [0,1]\n",
        "                    rgb_true = np.clip(rgb_true, 0, 1)\n",
        "                    rgb_pred = np.clip(rgb_pred, 0, 1)\n",
        "\n",
        "                    # Calculate metrics with proper parameters\n",
        "                    psnr_val = psnr(rgb_true, rgb_pred, data_range=1.0)\n",
        "\n",
        "                    # FIXED SSIM calculation - specify channel_axis and smaller win_size if needed\n",
        "                    min_dim = min(rgb_true.shape[0], rgb_true.shape[1])\n",
        "                    win_size = min(7, min_dim) if min_dim >= 3 else 3\n",
        "\n",
        "                    if min_dim >= 3:  # Only calculate SSIM if image is large enough\n",
        "                        ssim_val = ssim(rgb_true, rgb_pred,\n",
        "                                      channel_axis=2,  # Specify channel axis\n",
        "                                      data_range=1.0,\n",
        "                                      win_size=win_size)\n",
        "                        ssim_scores.append(ssim_val)\n",
        "\n",
        "                    psnr_scores.append(psnr_val)\n",
        "                    sample_count += 1\n",
        "\n",
        "                except Exception as e:\n",
        "                    print(f\"Error evaluating sample {sample_count}: {e}\")\n",
        "                    continue\n",
        "\n",
        "    if psnr_scores:\n",
        "        print(f\"PSNR: {np.mean(psnr_scores):.2f} ± {np.std(psnr_scores):.2f}\")\n",
        "    if ssim_scores:\n",
        "        print(f\"SSIM: {np.mean(ssim_scores):.3f} ± {np.std(ssim_scores):.3f}\")\n",
        "    else:\n",
        "        print(\"SSIM: Could not calculate (images too small)\")\n",
        "\n",
        "    return psnr_scores, ssim_scores"
      ],
      "metadata": {
        "id": "59tvvTLA9hI3"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# VIDEO COLORIZER\n",
        "class VideoColorizer:\n",
        "    def __init__(self, model_path):\n",
        "        self.model = ColorizationModel().to(device)\n",
        "        if os.path.exists(model_path):\n",
        "            self.model.load_state_dict(torch.load(model_path, map_location=device))\n",
        "        else:\n",
        "            print(f\"Warning: Model file {model_path} not found. Using untrained model.\")\n",
        "        self.model.eval()\n",
        "\n",
        "    def colorize_frame(self, gray_frame):\n",
        "        \"\"\"Colorize a single grayscale frame\"\"\"\n",
        "        try:\n",
        "            # Resize and normalize\n",
        "            frame_resized = cv2.resize(gray_frame, (256, 256))\n",
        "            frame_norm = frame_resized.astype(np.float32) / 255.0\n",
        "\n",
        "            # Convert to LAB L channel\n",
        "            L = frame_norm * 100.0  # [0, 100]\n",
        "            L_norm = L / 100.0 * 2.0 - 1.0  # [-1, 1]\n",
        "\n",
        "            # Convert to tensor\n",
        "            L_tensor = torch.FloatTensor(L_norm).unsqueeze(0).unsqueeze(0).to(device)\n",
        "\n",
        "            # Predict AB channels\n",
        "            with torch.no_grad():\n",
        "                pred_AB = self.model(L_tensor).cpu().squeeze().numpy()\n",
        "\n",
        "            # Denormalize\n",
        "            AB_denorm = pred_AB * 128.0\n",
        "            L_denorm = (L_norm + 1.0) / 2.0 * 100.0\n",
        "\n",
        "            # Clamp values\n",
        "            L_denorm = np.clip(L_denorm, 0, 100)\n",
        "            AB_denorm = np.clip(AB_denorm, -128, 127)\n",
        "\n",
        "            # Combine LAB\n",
        "            lab_frame = np.zeros((256, 256, 3))\n",
        "            lab_frame[:,:,0] = L_denorm\n",
        "            lab_frame[:,:,1:] = AB_denorm.transpose(1,2,0)\n",
        "\n",
        "            # Convert to RGB\n",
        "            rgb_frame = color.lab2rgb(lab_frame)\n",
        "            rgb_frame = np.clip(rgb_frame * 255, 0, 255).astype(np.uint8)\n",
        "\n",
        "            return rgb_frame\n",
        "\n",
        "        except Exception as e:\n",
        "            print(f\"Error colorizing frame: {e}\")\n",
        "            # Return grayscale frame as RGB fallback\n",
        "            rgb_fallback = cv2.cvtColor(cv2.resize(gray_frame, (256, 256)), cv2.COLOR_GRAY2RGB)\n",
        "            return rgb_fallback\n",
        "\n",
        "    def colorize_video(self, input_path, output_path, batch_size=4):\n",
        "        \"\"\"Colorize an entire video file\"\"\"\n",
        "        # Open input video\n",
        "        cap = cv2.VideoCapture(input_path)\n",
        "\n",
        "        if not cap.isOpened():\n",
        "            print(f\"Error: Could not open video {input_path}\")\n",
        "            return\n",
        "\n",
        "        # Get video properties\n",
        "        fps = int(cap.get(cv2.CAP_PROP_FPS))\n",
        "        width = int(cap.get(cv2.CAP_PROP_FRAME_WIDTH))\n",
        "        height = int(cap.get(cv2.CAP_PROP_FRAME_HEIGHT))\n",
        "        total_frames = int(cap.get(cv2.CAP_PROP_FRAME_COUNT))\n",
        "\n",
        "        # Setup video writer\n",
        "        fourcc = cv2.VideoWriter_fourcc(*'mp4v')\n",
        "        out = cv2.VideoWriter(output_path, fourcc, fps, (width, height))\n",
        "\n",
        "        print(f\"Processing {total_frames} frames at {fps} FPS...\")\n",
        "\n",
        "        with tqdm(total=total_frames, desc=\"Colorizing video\") as pbar:\n",
        "            while True:\n",
        "                ret, frame = cap.read()\n",
        "                if not ret:\n",
        "                    break\n",
        "\n",
        "                # Convert to grayscale\n",
        "                gray_frame = cv2.cvtColor(frame, cv2.COLOR_BGR2GRAY)\n",
        "\n",
        "                # Colorize at 256x256 then resize back\n",
        "                colorized_256 = self.colorize_frame(gray_frame)\n",
        "                colorized_full = cv2.resize(colorized_256, (width, height))\n",
        "\n",
        "                # Convert RGB to BGR for OpenCV\n",
        "                bgr_frame = cv2.cvtColor(colorized_full, cv2.COLOR_RGB2BGR)\n",
        "                out.write(bgr_frame)\n",
        "                pbar.update(1)\n",
        "\n",
        "        cap.release()\n",
        "        out.release()\n",
        "        print(f\"Video saved to {output_path}\")"
      ],
      "metadata": {
        "id": "yPyOnUgG9ldc"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# TRAINING FUNCTION\n",
        "def train_model(data_dir='data/coco/images/val2017'):\n",
        "    \"\"\"Train the improved colorization model\"\"\"\n",
        "\n",
        "    # Check if data directory exists\n",
        "    if not os.path.exists(data_dir):\n",
        "        print(f\"Data directory {data_dir} not found!\")\n",
        "        print(\"Please ensure you have downloaded the COCO dataset.\")\n",
        "        return None, [], []\n",
        "\n",
        "    try:\n",
        "        # Initialize improved dataset\n",
        "        full_dataset = ColorizationDataset(data_dir, size=256, augment=True)\n",
        "\n",
        "        if len(full_dataset) == 0:\n",
        "            print(\"No images found in dataset!\")\n",
        "            return None, [], []\n",
        "\n",
        "        train_size = int(0.8 * len(full_dataset))\n",
        "        val_size = len(full_dataset) - train_size\n",
        "        train_ds, val_ds = random_split(full_dataset, [train_size, val_size])\n",
        "\n",
        "        # Model and loss\n",
        "        model = ColorizationModel().to(device)\n",
        "        criterion = PerceptualLoss().to(device)\n",
        "\n",
        "        # Different learning rates for encoder and decoder\n",
        "        encoder_params = []\n",
        "        decoder_params = []\n",
        "\n",
        "        for name, param in model.named_parameters():\n",
        "            if 'encoder' in name:\n",
        "                encoder_params.append(param)\n",
        "            else:\n",
        "                decoder_params.append(param)\n",
        "\n",
        "        optimizer = optim.Adam([\n",
        "            {'params': encoder_params, 'lr': 1e-5},  # Lower LR for pretrained encoder\n",
        "            {'params': decoder_params, 'lr': 1e-4}   # Higher LR for decoder\n",
        "        ])\n",
        "\n",
        "        # Learning rate scheduler\n",
        "        scheduler = optim.lr_scheduler.ReduceLROnPlateau(optimizer, patience=5, factor=0.5)\n",
        "\n",
        "        # Data loaders\n",
        "        train_loader = DataLoader(train_ds, batch_size=8, shuffle=True, num_workers=2, pin_memory=True)\n",
        "        val_loader = DataLoader(val_ds, batch_size=8, shuffle=False, num_workers=2, pin_memory=True)\n",
        "\n",
        "        # Training loop\n",
        "        num_epochs = 100\n",
        "        best_val_loss = float('inf')\n",
        "        train_losses, val_losses = [], []\n",
        "\n",
        "        for epoch in range(1, num_epochs + 1):\n",
        "            # Training\n",
        "            model.train()\n",
        "            running_loss = 0.0\n",
        "            num_batches = 0\n",
        "\n",
        "            for L, AB in tqdm(train_loader, desc=f\"Epoch {epoch} [Train]\"):\n",
        "                try:\n",
        "                    L, AB = L.to(device), AB.to(device)\n",
        "\n",
        "                    optimizer.zero_grad()\n",
        "                    pred_AB = model(L)\n",
        "\n",
        "                    # Combine L and AB for perceptual loss\n",
        "                    pred_LAB = torch.cat([L, pred_AB], dim=1)\n",
        "                    target_LAB = torch.cat([L, AB], dim=1)\n",
        "\n",
        "                    loss = criterion(pred_LAB, target_LAB)\n",
        "                    loss.backward()\n",
        "                    optimizer.step()\n",
        "\n",
        "                    running_loss += loss.item()\n",
        "                    num_batches += 1\n",
        "                except Exception as e:\n",
        "                    print(f\"Training batch error: {e}\")\n",
        "                    continue\n",
        "\n",
        "            if num_batches > 0:\n",
        "                train_loss = running_loss / num_batches\n",
        "                train_losses.append(train_loss)\n",
        "            else:\n",
        "                train_losses.append(float('inf'))\n",
        "\n",
        "            # Validation\n",
        "            model.eval()\n",
        "            val_loss = 0.0\n",
        "            num_val_batches = 0\n",
        "\n",
        "            with torch.no_grad():\n",
        "                for L, AB in val_loader:\n",
        "                    try:\n",
        "                        L, AB = L.to(device), AB.to(device)\n",
        "                        pred_AB = model(L)\n",
        "\n",
        "                        pred_LAB = torch.cat([L, pred_AB], dim=1)\n",
        "                        target_LAB = torch.cat([L, AB], dim=1)\n",
        "\n",
        "                        loss = criterion(pred_LAB, target_LAB)\n",
        "                        val_loss += loss.item()\n",
        "                        num_val_batches += 1\n",
        "                    except Exception as e:\n",
        "                        print(f\"Validation batch error: {e}\")\n",
        "                        continue\n",
        "\n",
        "            if num_val_batches > 0:\n",
        "                val_loss /= num_val_batches\n",
        "                val_losses.append(val_loss)\n",
        "            else:\n",
        "                val_losses.append(float('inf'))\n",
        "\n",
        "            print(f\"Epoch {epoch}: Train={train_losses[-1]:.4f}, Val={val_losses[-1]:.4f}\")\n",
        "\n",
        "            # Save best model\n",
        "            if val_losses[-1] < best_val_loss:\n",
        "                best_val_loss = val_losses[-1]\n",
        "                torch.save(model.state_dict(), 'best_colorization_model.pth')\n",
        "                print(f\"New best model saved with val_loss: {best_val_loss:.4f}\")\n",
        "\n",
        "            scheduler.step(val_losses[-1])\n",
        "\n",
        "        return model, train_losses, val_losses\n",
        "\n",
        "    except Exception as e:\n",
        "        print(f\"Training error: {e}\")\n",
        "        return None, [], []"
      ],
      "metadata": {
        "id": "v8MmJKFW9qlT"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# QUICK TEST FUNCTION\n",
        "def quick_test_colorization(model_path, image_path):\n",
        "    \"\"\"Quick test function for single image\"\"\"\n",
        "    if not os.path.exists(model_path):\n",
        "        print(f\"Model file {model_path} not found!\")\n",
        "        return\n",
        "\n",
        "    if not os.path.exists(image_path):\n",
        "        print(f\"Image file {image_path} not found!\")\n",
        "        return\n",
        "\n",
        "    colorizer = VideoColorizer(model_path)\n",
        "\n",
        "    # Load and convert image to grayscale\n",
        "    img = cv2.imread(image_path)\n",
        "    if img is None:\n",
        "        print(f\"Could not load image {image_path}\")\n",
        "        return\n",
        "\n",
        "    gray = cv2.cvtColor(img, cv2.COLOR_BGR2GRAY)\n",
        "\n",
        "    # Colorize\n",
        "    colorized = colorizer.colorize_frame(gray)\n",
        "\n",
        "    # Display results\n",
        "    fig, axes = plt.subplots(1, 3, figsize=(15, 5))\n",
        "    axes[0].imshow(cv2.cvtColor(img, cv2.COLOR_BGR2RGB))\n",
        "    axes[0].set_title('Original')\n",
        "    axes[0].axis('off')\n",
        "\n",
        "    axes[1].imshow(gray, cmap='gray')\n",
        "    axes[1].set_title('Grayscale')\n",
        "    axes[1].axis('off')\n",
        "\n",
        "    axes[2].imshow(colorized)\n",
        "    axes[2].set_title('Colorized')\n",
        "    axes[2].axis('off')\n",
        "\n",
        "    plt.tight_layout()\n",
        "    plt.show()"
      ],
      "metadata": {
        "id": "5Ux6rXRS1WVm"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# STEP 1: TRAIN THE MODEL\n",
        "\n",
        "# Download COCO dataset (if not already done)\n",
        "!mkdir -p data/coco/images\n",
        "!wget -O data/coco/val2017.zip http://images.cocodataset.org/zips/val2017.zip\n",
        "!unzip -q data/coco/val2017.zip -d data/coco/images/\n",
        "\n",
        "# Train the improved model\n",
        "print(\"Starting training...\")\n",
        "model, train_losses, val_losses = train_model()\n",
        "\n",
        "# Plot improved training curves\n",
        "plt.figure(figsize=(12, 4))\n",
        "\n",
        "plt.subplot(1, 2, 1)\n",
        "plt.plot(train_losses, label='Train Loss', linewidth=2)\n",
        "plt.plot(val_losses, label='Val Loss', linewidth=2)\n",
        "plt.xlabel('Epoch')\n",
        "plt.ylabel('Loss')\n",
        "plt.title('Training Progress')\n",
        "plt.legend()\n",
        "plt.grid(True, alpha=0.3)\n",
        "\n",
        "plt.subplot(1, 2, 2)\n",
        "plt.plot(val_losses, 'r-', linewidth=2)\n",
        "plt.xlabel('Epoch')\n",
        "plt.ylabel('Validation Loss')\n",
        "plt.title('Validation Loss Trend')\n",
        "plt.grid(True, alpha=0.3)\n",
        "\n",
        "plt.tight_layout()\n",
        "plt.show()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "4GLRuzkHuKG3",
        "outputId": "de36bcd7-d77a-450b-d394-73e03bbb6883"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "--2025-06-27 17:56:50--  http://images.cocodataset.org/zips/val2017.zip\n",
            "Resolving images.cocodataset.org (images.cocodataset.org)... 52.217.236.145, 52.216.147.172, 3.5.25.90, ...\n",
            "Connecting to images.cocodataset.org (images.cocodataset.org)|52.217.236.145|:80... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 815585330 (778M) [application/zip]\n",
            "Saving to: ‘data/coco/val2017.zip’\n",
            "\n",
            "data/coco/val2017.z 100%[===================>] 777.80M  95.3MB/s    in 8.1s    \n",
            "\n",
            "2025-06-27 17:56:58 (95.6 MB/s) - ‘data/coco/val2017.zip’ saved [815585330/815585330]\n",
            "\n",
            "Starting training...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Downloading: \"https://download.pytorch.org/models/resnet50-11ad3fa6.pth\" to /root/.cache/torch/hub/checkpoints/resnet50-11ad3fa6.pth\n",
            "100%|██████████| 97.8M/97.8M [00:01<00:00, 81.5MB/s]\n",
            "Downloading: \"https://download.pytorch.org/models/vgg19-dcbb9e9d.pth\" to /root/.cache/torch/hub/checkpoints/vgg19-dcbb9e9d.pth\n",
            "100%|██████████| 548M/548M [00:07<00:00, 73.3MB/s]\n",
            "Epoch 1 [Train]:   0%|          | 0/500 [00:00<?, ?it/s]/tmp/ipython-input-5-3078761571.py:53: UserWarning: Conversion from CIE-LAB, via XYZ to sRGB color space resulted in 16 negative Z values that have been clipped to zero\n",
            "  rgb_img = color.lab2rgb(lab_img)\n",
            "/tmp/ipython-input-5-3078761571.py:53: UserWarning: Conversion from CIE-LAB, via XYZ to sRGB color space resulted in 11 negative Z values that have been clipped to zero\n",
            "  rgb_img = color.lab2rgb(lab_img)\n",
            "/tmp/ipython-input-5-3078761571.py:53: UserWarning: Conversion from CIE-LAB, via XYZ to sRGB color space resulted in 2 negative Z values that have been clipped to zero\n",
            "  rgb_img = color.lab2rgb(lab_img)\n",
            "Epoch 1 [Train]:   0%|          | 1/500 [00:57<7:59:17, 57.63s/it]/tmp/ipython-input-5-3078761571.py:53: UserWarning: Conversion from CIE-LAB, via XYZ to sRGB color space resulted in 1 negative Z values that have been clipped to zero\n",
            "  rgb_img = color.lab2rgb(lab_img)\n",
            "/tmp/ipython-input-5-3078761571.py:53: UserWarning: Conversion from CIE-LAB, via XYZ to sRGB color space resulted in 7 negative Z values that have been clipped to zero\n",
            "  rgb_img = color.lab2rgb(lab_img)\n",
            "Epoch 1 [Train]:   0%|          | 2/500 [01:51<7:39:23, 55.35s/it]/tmp/ipython-input-5-3078761571.py:53: UserWarning: Conversion from CIE-LAB, via XYZ to sRGB color space resulted in 5 negative Z values that have been clipped to zero\n",
            "  rgb_img = color.lab2rgb(lab_img)\n",
            "Epoch 1 [Train]:   1%|          | 3/500 [02:46<7:37:14, 55.20s/it]/tmp/ipython-input-5-3078761571.py:53: UserWarning: Conversion from CIE-LAB, via XYZ to sRGB color space resulted in 4 negative Z values that have been clipped to zero\n",
            "  rgb_img = color.lab2rgb(lab_img)\n",
            "/tmp/ipython-input-5-3078761571.py:53: UserWarning: Conversion from CIE-LAB, via XYZ to sRGB color space resulted in 8 negative Z values that have been clipped to zero\n",
            "  rgb_img = color.lab2rgb(lab_img)\n",
            "Epoch 1 [Train]:   1%|          | 4/500 [03:39<7:30:57, 54.55s/it]/tmp/ipython-input-5-3078761571.py:53: UserWarning: Conversion from CIE-LAB, via XYZ to sRGB color space resulted in 20 negative Z values that have been clipped to zero\n",
            "  rgb_img = color.lab2rgb(lab_img)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# STEP 2: EVALUATE MODEL PERFORMANCE\n",
        "\n",
        "# Load validation data for evaluation\n",
        "val_dataset = ColorizationDataset('data/coco/images/val2017', size=256, augment=False)\n",
        "val_loader = DataLoader(val_dataset, batch_size=8, shuffle=False)\n",
        "\n",
        "# Evaluate with PSNR and SSIM\n",
        "print(\"\\n Evaluating model performance...\")\n",
        "psnr_scores, ssim_scores = evaluate_model(model, val_loader, num_samples=50)\n"
      ],
      "metadata": {
        "id": "G8iZ9QrzuuP5",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 211
        },
        "outputId": "4896541e-268b-49b6-d28e-1c3ba505b44a"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "error",
          "ename": "NameError",
          "evalue": "name 'ColorizationDataset' is not defined",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[0;32m/tmp/ipython-input-1-1654423213.py\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0;31m# Load validation data for evaluation\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 4\u001b[0;31m \u001b[0mval_dataset\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mColorizationDataset\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'data/coco/images/val2017'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msize\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m256\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0maugment\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      5\u001b[0m \u001b[0mval_loader\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mDataLoader\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mval_dataset\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbatch_size\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m8\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mshuffle\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      6\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mNameError\u001b[0m: name 'ColorizationDataset' is not defined"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# STEP 3: VIDEO COLORIZATION DEMO\n",
        "\n",
        "# Initialize video colorizer\n",
        "colorizer = VideoColorizer('best_colorization_model.pth')\n",
        "\n",
        "# Demo 1: Process uploaded video\n",
        "print(\"\\nVideo Colorization Demo\")\n",
        "print(\"Upload a grayscale video file:\")\n",
        "\n",
        "from google.colab import files\n",
        "uploaded = files.upload()\n",
        "\n",
        "for filename in uploaded.keys():\n",
        "    if filename.lower().endswith(('.mp4', '.avi', '.mov', '.mkv')):\n",
        "        input_video = filename\n",
        "        output_video = f\"colorized_{filename}\"\n",
        "\n",
        "        print(f\"Processing: {filename}\")\n",
        "\n",
        "        # Colorize the video\n",
        "        colorizer.colorize_video(input_video, output_video, batch_size=4)\n",
        "\n",
        "        # Display first few frames for preview\n",
        "        cap = cv2.VideoCapture(input_video)\n",
        "        cap_out = cv2.VideoCapture(output_video)\n",
        "\n",
        "        fig, axes = plt.subplots(2, 4, figsize=(16, 8))\n",
        "        fig.suptitle('Video Colorization Preview (First 4 Frames)', fontsize=16)\n",
        "\n",
        "        for i in range(4):\n",
        "            ret_in, frame_in = cap.read()\n",
        "            ret_out, frame_out = cap_out.read()\n",
        "\n",
        "            if ret_in and ret_out:\n",
        "                # Convert to RGB for matplotlib\n",
        "                frame_in_rgb = cv2.cvtColor(frame_in, cv2.COLOR_BGR2RGB)\n",
        "                frame_out_rgb = cv2.cvtColor(frame_out, cv2.COLOR_BGR2RGB)\n",
        "\n",
        "                # Convert input to grayscale for display\n",
        "                frame_gray = cv2.cvtColor(frame_in, cv2.COLOR_BGR2GRAY)\n",
        "\n",
        "                axes[0, i].imshow(frame_gray, cmap='gray')\n",
        "                axes[0, i].set_title(f'Frame {i+1} (Grayscale)')\n",
        "                axes[0, i].axis('off')\n",
        "\n",
        "                axes[1, i].imshow(frame_out_rgb)\n",
        "                axes[1, i].set_title(f'Frame {i+1} (Colorized)')\n",
        "                axes[1, i].axis('off')\n",
        "\n",
        "        cap.release()\n",
        "        cap_out.release()\n",
        "        plt.tight_layout()\n",
        "        plt.show()\n",
        "\n",
        "        # Download colorized video\n",
        "        files.download(output_video)"
      ],
      "metadata": {
        "id": "EXxrmLYhuzlT"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# STEP 4: BATCH IMAGE PROCESSING\n",
        "\n",
        "def batch_colorize_images(image_folder, output_folder, model_path):\n",
        "    \"\"\"Colorize all images in a folder\"\"\"\n",
        "    os.makedirs(output_folder, exist_ok=True)\n",
        "    colorizer = VideoColorizer(model_path)\n",
        "\n",
        "    image_files = [f for f in os.listdir(image_folder)\n",
        "                   if f.lower().endswith(('.jpg', '.png', '.jpeg'))]\n",
        "\n",
        "    for img_file in tqdm(image_files, desc=\"Colorizing images\"):\n",
        "        img_path = os.path.join(image_folder, img_file)\n",
        "\n",
        "        # Load image\n",
        "        img = cv2.imread(img_path)\n",
        "        if img is None:\n",
        "            continue\n",
        "\n",
        "        # Convert to grayscale\n",
        "        gray = cv2.cvtColor(img, cv2.COLOR_BGR2GRAY)\n",
        "\n",
        "        # Colorize\n",
        "        colorized = colorizer.colorize_frame(gray)\n",
        "\n",
        "        # Resize back to original size\n",
        "        original_shape = img.shape[:2]\n",
        "        colorized_resized = cv2.resize(colorized,\n",
        "                                     (img.shape[1], img.shape[0]))\n",
        "\n",
        "        # Save\n",
        "        output_path = os.path.join(output_folder, f\"colorized_{img_file}\")\n",
        "        colorized_bgr = cv2.cvtColor(colorized_resized, cv2.COLOR_RGB2BGR)\n",
        "        cv2.imwrite(output_path, colorized_bgr)\n",
        "\n",
        "    print(f\"Batch processing complete! Check {output_folder}\")"
      ],
      "metadata": {
        "id": "e0Dpzq5zu5l2"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# STEP 5: REAL-TIME COLORIZATION\n",
        "\n",
        "def real_time_colorization(model_path, camera_id=0):\n",
        "    \"\"\"Real-time colorization from webcam\"\"\"\n",
        "    colorizer = VideoColorizer(model_path)\n",
        "    cap = cv2.VideoCapture(camera_id)\n",
        "\n",
        "    print(\"🎥 Real-time colorization started. Press 'q' to quit.\")\n",
        "\n",
        "    while True:\n",
        "        ret, frame = cap.read()\n",
        "        if not ret:\n",
        "            break\n",
        "\n",
        "        # Convert to grayscale\n",
        "        gray = cv2.cvtColor(frame, cv2.COLOR_BGR2GRAY)\n",
        "\n",
        "        # Colorize (resize for speed)\n",
        "        small_gray = cv2.resize(gray, (128, 128))\n",
        "        colorized_smally"
      ],
      "metadata": {
        "id": "TpXpAXEjvABM"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Mount Google Drive\n",
        "from google.colab import drive\n",
        "drive.mount('/content/drive')\n",
        "\n",
        "# Save model to Drive\n",
        "torch.save(model.state_dict(), '/content/drive/MyDrive/colorization_model.pth')\n",
        "print(\"Model is successfully Saved to drive\")\n",
        "\n",
        "# Load from Drive in future sessions\n",
        "# model.load_state_dict(torch.load('/content/drive/MyDrive/colorization_model.pth', map_location=device))"
      ],
      "metadata": {
        "id": "WVQU5J3mdxqn"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}